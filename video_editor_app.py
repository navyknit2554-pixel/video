import streamlit as st
import os
import tempfile
import numpy as np
import json
import subprocess
from pathlib import Path

st.set_page_config(
    page_title="íŒ¨ìŠ¤íŒŒì¸ë” ì˜ìƒ í¸ì§‘ê¸°",
    page_icon="ğŸ¬",
    layout="wide"
)

# â”€â”€ CSS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%);
        padding: 2rem; border-radius: 16px; text-align: center;
        margin-bottom: 2rem; color: white;
    }
    .main-header h1 { font-size: 2.2rem; margin: 0; font-weight: 800; }
    .main-header p  { opacity: .8; margin: .4rem 0 0; }
    .feature-card {
        background: #f8fafc; border: 1px solid #e2e8f0;
        border-radius: 12px; padding: 1.2rem; margin-bottom: 1rem;
    }
    .feature-card h4 { margin: 0 0 .5rem; color: #1e293b; }
    .step-badge {
        background: #0f3460; color: white;
        border-radius: 50%; width: 28px; height: 28px;
        display: inline-flex; align-items: center; justify-content: center;
        font-size: .85rem; font-weight: bold; margin-right: .5rem;
    }
    .status-processing { color: #f59e0b; font-weight: 600; }
    .status-done       { color: #10b981; font-weight: 600; }
</style>
""", unsafe_allow_html=True)

st.markdown("""
<div class="main-header">
  <h1>ğŸ¬ íŒ¨ìŠ¤íŒŒì¸ë” ì˜ìƒ í¸ì§‘ê¸°</h1>
  <p>ìë™ ì»·í¸ì§‘ Â· ìë§‰ Â· ë°°ê²½ìŒ Â· ì´ë¯¸ì§€ ì‚½ì… Â· ì¸ë¬¼ íŠ¸ë˜í‚¹</p>
</div>
""", unsafe_allow_html=True)

# â”€â”€ Session state ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for k, v in {
    "subtitles": [],           # [{start, end, text}]
    "images": [],              # [{start, end, path, position, opacity}]
    "processing": False,
    "output_path": None,
}.items():
    if k not in st.session_state:
        st.session_state[k] = v

# â”€â”€ helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def sec_to_ts(s):
    h, r = divmod(int(s), 3600)
    m, sec = divmod(r, 60)
    return f"{h:02d}:{m:02d}:{sec:02d}"

def run_ffmpeg(cmd, desc="ffmpeg"):
    """Run ffmpeg quietly, return (ok, stderr)."""
    result = subprocess.run(
        ["ffmpeg", "-y"] + cmd,
        capture_output=True, text=True
    )
    return result.returncode == 0, result.stderr

def get_video_duration(path):
    r = subprocess.run(
        ["ffprobe", "-v", "quiet", "-print_format", "json",
         "-show_format", path],
        capture_output=True, text=True
    )
    try:
        return float(json.loads(r.stdout)["format"]["duration"])
    except Exception:
        return 0

def get_audio_rms_per_chunk(video_path, chunk_ms=100):
    """Extract per-chunk RMS dB using ffmpeg â†’ raw pcm â†’ numpy."""
    try:
        import librosa, soundfile as sf
        tmp_wav = tempfile.mktemp(suffix=".wav")
        subprocess.run(
            ["ffmpeg", "-y", "-i", video_path,
             "-ar", "16000", "-ac", "1", tmp_wav],
            capture_output=True
        )
        y, sr = librosa.load(tmp_wav, sr=16000, mono=True)
        os.unlink(tmp_wav)
        chunk_samples = int(sr * chunk_ms / 1000)
        chunks = [y[i:i+chunk_samples] for i in range(0, len(y), chunk_samples)]
        rms_db = []
        for c in chunks:
            rms = np.sqrt(np.mean(c**2) + 1e-10)
            rms_db.append(20 * np.log10(rms))
        return rms_db, chunk_ms / 1000.0
    except Exception as e:
        st.error(f"ì˜¤ë””ì˜¤ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return [], 0.1

def detect_silent_intervals(rms_db, chunk_dur, threshold_db, min_silence_sec=0.5):
    """Return list of (start, end) silent intervals."""
    silent = []
    start = None
    for i, db in enumerate(rms_db):
        t = i * chunk_dur
        if db < threshold_db:
            if start is None:
                start = t
        else:
            if start is not None and (t - start) >= min_silence_sec:
                silent.append((start, t))
            start = None
    if start is not None and (len(rms_db) * chunk_dur - start) >= min_silence_sec:
        silent.append((start, len(rms_db) * chunk_dur))
    return silent

def silence_to_keep(duration, silent_intervals):
    """Invert silent intervals â†’ keep intervals."""
    keep, prev = [], 0.0
    for s, e in silent_intervals:
        if s > prev + 0.05:
            keep.append((prev, s))
        prev = e
    if prev < duration - 0.05:
        keep.append((prev, duration))
    return keep

def build_concat_filter(keep_intervals, video_path, output_path):
    """Use ffmpeg complex filter to cut & concat kept segments."""
    parts = []
    filter_v, filter_a = [], []
    for i, (s, e) in enumerate(keep_intervals):
        dur = e - s
        parts += ["-ss", str(s), "-t", str(dur), "-i", video_path]
        filter_v.append(f"[{i}:v]")
        filter_a.append(f"[{i}:a]")
    n = len(keep_intervals)
    filter_complex = (
        "".join(filter_v) + f"concat=n={n}:v=1:a=0[outv];"
        + "".join(filter_a) + f"concat=n={n}:v=0:a=1[outa]"
    )
    cmd = parts + [
        "-filter_complex", filter_complex,
        "-map", "[outv]", "-map", "[outa]",
        output_path
    ]
    return cmd

def detect_person_boxes(video_path, sample_fps=2):
    """Detect person bounding boxes using mediapipe pose."""
    try:
        import mediapipe as mp_lib
        import cv2
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS) or 30
        w   = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        h   = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        step  = max(1, int(fps / sample_fps))
        boxes = {}
        mp_pose = mp_lib.solutions.pose
        pose = mp_pose.Pose(static_image_mode=False,
                             model_complexity=0,
                             min_detection_confidence=0.5)
        frame_idx = 0
        while True:
            ret, frame = cap.read()
            if not ret: break
            if frame_idx % step == 0:
                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                res = pose.process(rgb)
                if res.pose_landmarks:
                    lms = res.pose_landmarks.landmark
                    xs = [l.x for l in lms if l.visibility > 0.3]
                    ys = [l.y for l in lms if l.visibility > 0.3]
                    if xs and ys:
                        boxes[frame_idx] = (
                            max(0, min(xs) - 0.1),
                            max(0, min(ys) - 0.1),
                            min(1, max(xs) + 0.1),
                            min(1, max(ys) + 0.1)
                        )
            frame_idx += 1
        cap.release()
        pose.close()
        return boxes, w, h, fps, total
    except Exception as e:
        return {}, 0, 0, 30, 0

def apply_tracking(input_path, output_path, track_width_ratio=0.6,
                   smooth_frames=15, progress_cb=None):
    """Crop video to follow detected person."""
    import cv2
    boxes, W, H, fps, total = detect_person_boxes(input_path, sample_fps=3)
    if not boxes:
        return False, "ì¸ë¬¼ì„ ê°ì§€í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
    # Interpolate boxes for every frame
    all_frames = sorted(boxes.keys())
    cx_arr = np.array([0.5*(boxes[f][0]+boxes[f][2]) for f in all_frames])
    cy_arr = np.array([0.5*(boxes[f][1]+boxes[f][3]) for f in all_frames])
    full_cx = np.interp(range(total), all_frames, cx_arr)
    full_cy = np.interp(range(total), all_frames, cy_arr)
    # Smooth
    kernel = np.ones(smooth_frames) / smooth_frames
    full_cx = np.convolve(full_cx, kernel, mode='same')
    full_cy = np.convolve(full_cy, kernel, mode='same')

    crop_w = int(W * track_width_ratio)
    crop_h = int(crop_w * H / W)

    cap = cv2.VideoCapture(input_path)
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    tmp_no_audio = tempfile.mktemp(suffix=".mp4")
    out = cv2.VideoWriter(tmp_no_audio, fourcc, fps, (crop_w, crop_h))
    idx = 0
    while True:
        ret, frame = cap.read()
        if not ret: break
        cx = int(np.clip(full_cx[idx] * W, crop_w//2, W - crop_w//2))
        cy = int(np.clip(full_cy[idx] * H, crop_h//2, H - crop_h//2))
        x1, y1 = cx - crop_w//2, cy - crop_h//2
        x2, y2 = x1 + crop_w, y1 + crop_h
        x1, y1 = max(0, x1), max(0, y1)
        x2, y2 = min(W, x2), min(H, y2)
        cropped = frame[y1:y2, x1:x2]
        if cropped.shape[:2] != (crop_h, crop_w):
            cropped = cv2.resize(cropped, (crop_w, crop_h))
        out.write(cropped)
        idx += 1
        if progress_cb and idx % 30 == 0:
            progress_cb(idx / max(total, 1))
    cap.release()
    out.release()
    # Re-mux audio
    ok, err = run_ffmpeg([
        "-i", tmp_no_audio, "-i", input_path,
        "-c:v", "copy", "-map", "0:v:0", "-map", "1:a:0",
        "-shortest", output_path
    ])
    os.unlink(tmp_no_audio)
    return ok, err

def add_subtitles_ffmpeg(input_path, output_path, subtitles):
    """Burn subtitles using ffmpeg drawtext filter."""
    filter_parts = []
    for sub in subtitles:
        s   = float(sub["start"])
        e   = float(sub["end"])
        txt = sub["text"].replace("'", "\\'").replace(":", "\\:")
        filter_parts.append(
            f"drawtext=text='{txt}'"
            f":fontsize=36:fontcolor=white:borderw=3:bordercolor=black"
            f":x=(w-text_w)/2:y=h-80"
            f":enable='between(t,{s},{e})'"
        )
    vf = ",".join(filter_parts) if filter_parts else "null"
    ok, err = run_ffmpeg([
        "-i", input_path, "-vf", vf,
        "-c:a", "copy", output_path
    ])
    return ok, err

def add_bgm_ffmpeg(input_path, output_path, bgm_path, bgm_volume=0.3):
    """Mix background music into video."""
    ok, err = run_ffmpeg([
        "-i", input_path, "-i", bgm_path,
        "-filter_complex",
        f"[1:a]volume={bgm_volume},aloop=loop=-1:size=2e+09[bgm];"
        f"[0:a][bgm]amix=inputs=2:duration=first:dropout_transition=2[aout]",
        "-map", "0:v", "-map", "[aout]",
        "-c:v", "copy", output_path
    ])
    return ok, err

def overlay_images_ffmpeg(input_path, output_path, image_overlays):
    """Overlay images onto video at specified times."""
    if not image_overlays:
        return True, ""
    # Build filter_complex chain
    inputs = ["-i", input_path]
    for ov in image_overlays:
        inputs += ["-i", ov["path"]]
    n = len(image_overlays)
    # Build overlay chain
    prev = "0:v"
    filter_parts = []
    for i, ov in enumerate(image_overlays):
        idx   = i + 1
        s, e  = ov["start"], ov["end"]
        pos   = ov.get("position", "center")
        opacity = ov.get("opacity", 1.0)
        pos_map = {
            "center":       "(W-w)/2:(H-h)/2",
            "top-left":     "10:10",
            "top-right":    "W-w-10:10",
            "bottom-left":  "10:H-h-10",
            "bottom-right": "W-w-10:H-h-10",
        }
        xy = pos_map.get(pos, "(W-w)/2:(H-h)/2")
        out_label = f"v{i}"
        filter_parts.append(
            f"[{prev}][{idx}:v]"
            f"overlay={xy}:enable='between(t,{s},{e})'[{out_label}]"
        )
        prev = out_label
    filter_complex = ";".join(filter_parts)
    cmd = inputs + [
        "-filter_complex", filter_complex,
        "-map", f"[{prev}]", "-map", "0:a",
        "-c:a", "copy", output_path
    ]
    ok, err = run_ffmpeg(cmd)
    return ok, err

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MAIN UI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

col_left, col_right = st.columns([1, 1], gap="large")

# â”€â”€ LEFT: Upload & settings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with col_left:

    # 1. ì˜ìƒ ì—…ë¡œë“œ
    st.markdown('<div class="feature-card">', unsafe_allow_html=True)
    st.markdown("### <span class='step-badge'>1</span> ì˜ìƒ ì—…ë¡œë“œ", unsafe_allow_html=True)
    video_file = st.file_uploader("MP4, MOV, AVI, MKV", type=["mp4","mov","avi","mkv"])
    if video_file:
        tmp_video = tempfile.NamedTemporaryFile(
            delete=False, suffix=Path(video_file.name).suffix)
        tmp_video.write(video_file.read()); tmp_video.flush()
        VIDEO_PATH = tmp_video.name
        st.session_state["video_path"] = VIDEO_PATH
        dur = get_video_duration(VIDEO_PATH)
        st.session_state["duration"] = dur
        st.video(video_file)
        st.caption(f"ê¸¸ì´: {sec_to_ts(dur)}")
    st.markdown('</div>', unsafe_allow_html=True)

    if "video_path" not in st.session_state:
        st.info("ì˜ìƒì„ ë¨¼ì € ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.")
        st.stop()

    duration = st.session_state.get("duration", 0)

    # 2. ìë™ ì»·í¸ì§‘
    st.markdown('<div class="feature-card">', unsafe_allow_html=True)
    st.markdown("### <span class='step-badge'>2</span> ğŸ”‡ ìë™ ì»·í¸ì§‘ (ë¬´ìŒ ì œê±°)", unsafe_allow_html=True)
    use_cut = st.toggle("ìë™ ì»·í¸ì§‘ ì‚¬ìš©", value=False)
    if use_cut:
        db_thresh    = st.slider("ë¬´ìŒ ê¸°ì¤€ (dB)", -60, -10, -35)
        min_silence  = st.slider("ìµœì†Œ ë¬´ìŒ ê¸¸ì´ (ì´ˆ)", 0.2, 3.0, 0.5, 0.1)
        padding_ms   = st.slider("ì»· ì•ë’¤ ì—¬ìœ  (ms)", 0, 500, 100, 50)
        st.caption(f"ê¸°ì¤€: {db_thresh} dB ì´í•˜ êµ¬ê°„ì„ ì œê±°í•©ë‹ˆë‹¤.")
        st.session_state["cut_settings"] = {
            "enabled": True, "db_thresh": db_thresh,
            "min_silence": min_silence, "padding": padding_ms / 1000
        }
    else:
        st.session_state["cut_settings"] = {"enabled": False}
    st.markdown('</div>', unsafe_allow_html=True)

    # 3. ìë§‰
    st.markdown('<div class="feature-card">', unsafe_allow_html=True)
    st.markdown("### <span class='step-badge'>3</span> ğŸ’¬ ìë§‰ ì¶”ê°€", unsafe_allow_html=True)
    use_sub = st.toggle("ìë§‰ ì‚¬ìš©", value=False)
    if use_sub:
        with st.expander("ìë§‰ í•­ëª© ì¶”ê°€", expanded=True):
            c1, c2, c3 = st.columns([1,1,2])
            sub_start = c1.number_input("ì‹œì‘(ì´ˆ)", 0.0, duration, 0.0, 0.5, key="ss")
            sub_end   = c2.number_input("ë(ì´ˆ)",   0.0, duration, 3.0, 0.5, key="se")
            sub_text  = c3.text_input("ìë§‰ ë‚´ìš©", key="st")
            if st.button("â• ìë§‰ ì¶”ê°€"):
                if sub_text.strip():
                    st.session_state["subtitles"].append(
                        {"start": sub_start, "end": sub_end, "text": sub_text.strip()}
                    )
                    st.success(f"ì¶”ê°€ë¨: {sub_text}")
        if st.session_state["subtitles"]:
            st.markdown("**ë“±ë¡ëœ ìë§‰:**")
            to_del = []
            for i, s in enumerate(st.session_state["subtitles"]):
                cols = st.columns([3,1])
                cols[0].markdown(
                    f"`{sec_to_ts(s['start'])}` â†’ `{sec_to_ts(s['end'])}` &nbsp; **{s['text']}**",
                    unsafe_allow_html=True
                )
                if cols[1].button("ğŸ—‘", key=f"del_sub_{i}"):
                    to_del.append(i)
            for i in reversed(to_del):
                st.session_state["subtitles"].pop(i)
    st.markdown('</div>', unsafe_allow_html=True)

    # 4. ë°°ê²½ìŒ
    st.markdown('<div class="feature-card">', unsafe_allow_html=True)
    st.markdown("### <span class='step-badge'>4</span> ğŸµ ë°°ê²½ìŒ ì‚½ì…", unsafe_allow_html=True)
    use_bgm = st.toggle("ë°°ê²½ìŒ ì‚¬ìš©", value=False)
    if use_bgm:
        bgm_file = st.file_uploader("ë°°ê²½ìŒ íŒŒì¼ (MP3, WAV)", type=["mp3","wav"], key="bgm")
        bgm_vol  = st.slider("ë°°ê²½ìŒ ë³¼ë¥¨", 0.0, 1.0, 0.3, 0.05)
        if bgm_file:
            tmp_bgm = tempfile.NamedTemporaryFile(
                delete=False, suffix=Path(bgm_file.name).suffix)
            tmp_bgm.write(bgm_file.read()); tmp_bgm.flush()
            st.session_state["bgm_path"]   = tmp_bgm.name
            st.session_state["bgm_volume"] = bgm_vol
            st.audio(bgm_file)
        elif "bgm_path" in st.session_state:
            st.session_state["bgm_volume"] = bgm_vol
    st.markdown('</div>', unsafe_allow_html=True)

    # 5. ì´ë¯¸ì§€ ì‚½ì…
    st.markdown('<div class="feature-card">', unsafe_allow_html=True)
    st.markdown("### <span class='step-badge'>5</span> ğŸ–¼ ì´ë¯¸ì§€ ì‚½ì…", unsafe_allow_html=True)
    use_img = st.toggle("ì´ë¯¸ì§€ ì‚½ì… ì‚¬ìš©", value=False)
    if use_img:
        img_file = st.file_uploader("ì´ë¯¸ì§€ (PNG, JPG)", type=["png","jpg","jpeg"], key="img")
        if img_file:
            c1, c2, c3, c4 = st.columns(4)
            img_start = c1.number_input("ì‹œì‘(ì´ˆ)", 0.0, duration, 0.0, 0.5, key="is")
            img_end   = c2.number_input("ë(ì´ˆ)",   0.0, duration, 3.0, 0.5, key="ie")
            img_pos   = c3.selectbox("ìœ„ì¹˜", ["center","top-left","top-right","bottom-left","bottom-right"])
            img_scale = c4.slider("í¬ê¸° (%)", 10, 100, 30)
            if st.button("â• ì´ë¯¸ì§€ ì¶”ê°€"):
                from PIL import Image
                tmp_img = tempfile.NamedTemporaryFile(delete=False, suffix=".png")
                pil = Image.open(img_file)
                # Resize
                w, h = pil.size
                new_w = max(50, int(w * img_scale / 100))
                new_h = int(h * new_w / w)
                pil = pil.resize((new_w, new_h), Image.LANCZOS)
                pil.save(tmp_img.name)
                st.session_state["images"].append({
                    "start": img_start, "end": img_end,
                    "path": tmp_img.name, "position": img_pos,
                    "opacity": 1.0
                })
                st.success("ì´ë¯¸ì§€ ì¶”ê°€ë¨!")
        if st.session_state["images"]:
            st.markdown(f"**ë“±ë¡ëœ ì´ë¯¸ì§€:** {len(st.session_state['images'])}ê°œ")
            if st.button("ğŸ—‘ ì „ì²´ ì‚­ì œ"):
                st.session_state["images"] = []
    st.markdown('</div>', unsafe_allow_html=True)

    # 6. ì¸ë¬¼ íŠ¸ë˜í‚¹
    st.markdown('<div class="feature-card">', unsafe_allow_html=True)
    st.markdown("### <span class='step-badge'>6</span> ğŸ¯ ì¸ë¬¼ ìë™ íŠ¸ë˜í‚¹", unsafe_allow_html=True)
    use_track = st.toggle("ì¸ë¬¼ íŠ¸ë˜í‚¹ ì‚¬ìš©", value=False)
    if use_track:
        st.info("MediaPipe Poseë¡œ ì¸ë¬¼ì˜ ìœ„ì¹˜ë¥¼ ì¶”ì í•˜ì—¬ í™”ë©´ì„ ìë™ìœ¼ë¡œ ë”°ë¼ê°‘ë‹ˆë‹¤.")
        track_width = st.slider("í™”ë©´ í¬ë¡­ ë¹„ìœ¨ (%)", 40, 100, 60) / 100
        track_smooth = st.slider("ì›€ì§ì„ ë¶€ë“œëŸ¬ì›€", 5, 60, 20)
        st.session_state["track_settings"] = {
            "enabled": True,
            "width_ratio": track_width,
            "smooth": track_smooth
        }
    else:
        st.session_state["track_settings"] = {"enabled": False}
    st.markdown('</div>', unsafe_allow_html=True)

# â”€â”€ RIGHT: Process & preview â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with col_right:
    st.markdown("### ğŸš€ í¸ì§‘ ì²˜ë¦¬")

    # Summary
    enabled = []
    cs = st.session_state.get("cut_settings", {})
    ts = st.session_state.get("track_settings", {})
    if cs.get("enabled"): enabled.append("âœ… ìë™ ì»·í¸ì§‘")
    if use_sub and st.session_state["subtitles"]:
        enabled.append(f"âœ… ìë§‰ {len(st.session_state['subtitles'])}ê°œ")
    if use_bgm and "bgm_path" in st.session_state: enabled.append("âœ… ë°°ê²½ìŒ")
    if use_img and st.session_state["images"]:
        enabled.append(f"âœ… ì´ë¯¸ì§€ {len(st.session_state['images'])}ê°œ")
    if ts.get("enabled"): enabled.append("âœ… ì¸ë¬¼ íŠ¸ë˜í‚¹")

    if enabled:
        st.markdown("**ì ìš©ë  í¸ì§‘:**")
        for e in enabled:
            st.markdown(f"- {e}")
    else:
        st.info("ì™¼ìª½ì—ì„œ ì›í•˜ëŠ” í¸ì§‘ ê¸°ëŠ¥ì„ í™œì„±í™”í•˜ì„¸ìš”.")

    st.markdown("---")

    if st.button("ğŸ¬ í¸ì§‘ ì‹œì‘", type="primary", use_container_width=True):
        if not enabled:
            st.warning("ì ìš©í•  í¸ì§‘ ê¸°ëŠ¥ì„ í•˜ë‚˜ ì´ìƒ ì„ íƒí•´ì£¼ì„¸ìš”.")
        else:
            progress_bar = st.progress(0, text="í¸ì§‘ ì¤€ë¹„ ì¤‘...")
            log_area = st.empty()
            current = st.session_state["video_path"]

            try:
                step_total = len(enabled)
                step_done  = 0

                def upd(pct, msg):
                    progress_bar.progress(
                        min(0.99, (step_done / step_total) + pct / step_total),
                        text=msg
                    )

                # STEP A: ìë™ ì»·í¸ì§‘
                if cs.get("enabled"):
                    log_area.markdown("ğŸ” **ë¬´ìŒ êµ¬ê°„ ë¶„ì„ ì¤‘...**")
                    rms_db, chunk_dur = get_audio_rms_per_chunk(current)
                    if rms_db:
                        silents = detect_silent_intervals(
                            rms_db, chunk_dur,
                            cs["db_thresh"], cs["min_silence"]
                        )
                        # Add padding
                        pad = cs.get("padding", 0.1)
                        silents = [
                            (max(0, s+pad), e-pad)
                            for s, e in silents if (e-pad) > (s+pad)
                        ]
                        keeps = silence_to_keep(duration, silents)
                        log_area.markdown(
                            f"âœ‚ï¸ **ì»·í¸ì§‘:** ë¬´ìŒ {len(silents)}ê°œ êµ¬ê°„ ì œê±° "
                            f"({len(keeps)}ê°œ êµ¬ê°„ ìœ ì§€)"
                        )
                        if keeps:
                            tmp_out = tempfile.mktemp(suffix=".mp4")
                            cmd = build_concat_filter(keeps, current, tmp_out)
                            ok, err = run_ffmpeg(cmd)
                            if ok:
                                current = tmp_out
                            else:
                                st.warning(f"ì»·í¸ì§‘ ì¤‘ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {err[-200:]}")
                    step_done += 1
                    upd(1.0, "ì»·í¸ì§‘ ì™„ë£Œ")

                # STEP B: ì¸ë¬¼ íŠ¸ë˜í‚¹
                if ts.get("enabled"):
                    log_area.markdown("ğŸ¯ **ì¸ë¬¼ íŠ¸ë˜í‚¹ ì²˜ë¦¬ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)**")
                    tmp_out = tempfile.mktemp(suffix=".mp4")
                    ok, err = apply_tracking(
                        current, tmp_out,
                        track_width_ratio=ts["width_ratio"],
                        smooth_frames=ts["smooth"],
                        progress_cb=lambda p: upd(p, f"íŠ¸ë˜í‚¹ ì²˜ë¦¬ ì¤‘... {int(p*100)}%")
                    )
                    if ok:
                        current = tmp_out
                    else:
                        st.warning(f"íŠ¸ë˜í‚¹ ì˜¤ë¥˜ (ê³„ì†): {err[-200:]}")
                    step_done += 1
                    upd(1.0, "íŠ¸ë˜í‚¹ ì™„ë£Œ")

                # STEP C: ì´ë¯¸ì§€ ì˜¤ë²„ë ˆì´
                if use_img and st.session_state["images"]:
                    log_area.markdown("ğŸ–¼ **ì´ë¯¸ì§€ í•©ì„± ì¤‘...**")
                    tmp_out = tempfile.mktemp(suffix=".mp4")
                    ok, err = overlay_images_ffmpeg(
                        current, tmp_out, st.session_state["images"]
                    )
                    if ok:
                        current = tmp_out
                    else:
                        st.warning(f"ì´ë¯¸ì§€ í•©ì„± ì˜¤ë¥˜ (ê³„ì†): {err[-200:]}")
                    step_done += 1
                    upd(1.0, "ì´ë¯¸ì§€ í•©ì„± ì™„ë£Œ")

                # STEP D: ë°°ê²½ìŒ
                if use_bgm and "bgm_path" in st.session_state:
                    log_area.markdown("ğŸµ **ë°°ê²½ìŒ í•©ì„± ì¤‘...**")
                    tmp_out = tempfile.mktemp(suffix=".mp4")
                    ok, err = add_bgm_ffmpeg(
                        current, tmp_out,
                        st.session_state["bgm_path"],
                        st.session_state.get("bgm_volume", 0.3)
                    )
                    if ok:
                        current = tmp_out
                    else:
                        st.warning(f"ë°°ê²½ìŒ ì˜¤ë¥˜ (ê³„ì†): {err[-200:]}")
                    step_done += 1
                    upd(1.0, "ë°°ê²½ìŒ í•©ì„± ì™„ë£Œ")

                # STEP E: ìë§‰
                if use_sub and st.session_state["subtitles"]:
                    log_area.markdown("ğŸ’¬ **ìë§‰ ì…íˆëŠ” ì¤‘...**")
                    tmp_out = tempfile.mktemp(suffix=".mp4")
                    ok, err = add_subtitles_ffmpeg(
                        current, tmp_out, st.session_state["subtitles"]
                    )
                    if ok:
                        current = tmp_out
                    else:
                        st.warning(f"ìë§‰ ì˜¤ë¥˜ (ê³„ì†): {err[-200:]}")
                    step_done += 1
                    upd(1.0, "ìë§‰ ì™„ë£Œ")

                # ìµœì¢… ì¶œë ¥ íŒŒì¼ì„ /mnt/user-data/outputs ìœ¼ë¡œ ë³µì‚¬
                final_path = "/mnt/user-data/outputs/edited_video.mp4"
                import shutil
                shutil.copy2(current, final_path)
                st.session_state["output_path"] = final_path

                progress_bar.progress(1.0, text="âœ… í¸ì§‘ ì™„ë£Œ!")
                log_area.empty()

            except Exception as ex:
                st.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {ex}")
                import traceback; traceback.print_exc()

    # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° & ë‹¤ìš´ë¡œë“œ
    if st.session_state.get("output_path") and os.path.exists(st.session_state["output_path"]):
        st.success("ğŸ‰ í¸ì§‘ ì™„ë£Œ!")
        new_dur = get_video_duration(st.session_state["output_path"])
        st.caption(f"ê²°ê³¼ ì˜ìƒ ê¸¸ì´: {sec_to_ts(new_dur)}")
        st.video(st.session_state["output_path"])
        with open(st.session_state["output_path"], "rb") as f:
            st.download_button(
                "â¬‡ï¸ í¸ì§‘ëœ ì˜ìƒ ë‹¤ìš´ë¡œë“œ",
                f, "edited_video.mp4", "video/mp4",
                use_container_width=True
            )

    # ì´ˆê¸°í™”
    st.markdown("---")
    if st.button("ğŸ”„ ì „ì²´ ì´ˆê¸°í™”", use_container_width=True):
        for k in ["video_path","output_path","subtitles","images","bgm_path","duration"]:
            if k in st.session_state:
                del st.session_state[k]
        st.session_state["subtitles"] = []
        st.session_state["images"]    = []
        st.rerun()
